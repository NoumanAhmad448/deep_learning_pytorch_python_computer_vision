{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def change_device(tensor: torch.Tensor, device=\"cuda\"):\n",
        "  return tensor.to(device)\n",
        "\n",
        "def setup_device():\n",
        "    return \"cuda\" if  torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def change_dtype(tensor:torch.Tensor, dtype=torch.float):\n",
        "  \"\"\"\n",
        "    dtype must have the value mentioned in the torch documentation\n",
        "    e.g. torch.float , torch.LongTensor\n",
        "  \"\"\"\n",
        "  return tensor.to(dtype)\n",
        "def show_image(image, label=None, is_squeeze_req:bool=False):\n",
        "  \"\"\"\n",
        "    show image on the screen\n",
        "    require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  if label is None:\n",
        "    label = \"Image Caption\"\n",
        "  plt.figure(figsize=(4,4))\n",
        "  plt.imshow(convert_to_numpy(image))\n",
        "  plt.title(label)\n",
        "  plt.axis(False)\n",
        "\n",
        "def convert_to_tensor(array, is_change_dtype=False, is_change_device=False, to_device=\"cpu\")->torch.Tensor:\n",
        "  \"\"\"\n",
        "  convert numpy array to tensor\n",
        "  is_change_dtype=True will change the dtype to float\n",
        "  \"\"\"\n",
        "\n",
        "  tensor = None\n",
        "  if torch.is_tensor(array):\n",
        "     tensor = array\n",
        "  else:\n",
        "     tensor = torch.from_numpy(array)\n",
        "  \n",
        "  if is_change_dtype: \n",
        "    tensor = change_dtype(tensor)\n",
        "  \n",
        "  if is_change_device: \n",
        "    tensor = change_device(tensor, device=to_device)\n",
        "  \n",
        "  return tensor\n",
        "\n",
        "# convert_to_tensor(random.rand(2,3),is_change_dtype=True)"
      ],
      "metadata": {
        "id": "vCIJzVZCYz0d"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_numpy(tensor:torch.Tensor):\n",
        "  \"\"\"\n",
        "  convert tensor to numpy\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return tensor.cpu() if tensor.ndim > 1 else \"tensor ndim must be greater than 1\"\n",
        "      \n",
        "  except:\n",
        "    return \"Something went wrong while converting tensor to numpy\""
      ],
      "metadata": {
        "id": "5_qUymunVkQL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, label=None, cmap:str=\"gray\",figsize:tuple = (4,4), is_squeeze_not_req:bool=False):\n",
        "  \"\"\"\n",
        "    show image on the screen\n",
        "    require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  if label is None:\n",
        "    label = \"Image Caption\"\n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.imshow(convert_to_numpy(image if is_squeeze_not_req else image.squeeze()), cmap=cmap)\n",
        "  plt.title(label)\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "KaniAV6mVGqm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_multi_images(data: torch.Tensor, figsize:tuple=(9,9), rows:int=3, cols:int=3, classes=None,\n",
        "                      cmap=\"gray\", is_require_squeeze:bool=True):\n",
        "  \"\"\"\n",
        "  classes list|dict\n",
        "  require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  DEFAULT_LABEL = None\n",
        "  if classes is None:\n",
        "    DEFAULT_LABEL = \"Image Caption\"\n",
        "  \n",
        "  figure = plt.figure(figsize=figsize)\n",
        "  cols, rows = cols, rows\n",
        "  for i in range(1, cols * rows + 1):\n",
        "      sample_idx = torch.randint(len(data), size=(1,)).item()\n",
        "      img, label = data[sample_idx]\n",
        "      figure.add_subplot(rows, cols, i)\n",
        "      plt.title(DEFAULT_LABEL if classes is None else classes[label])\n",
        "      plt.axis(False)\n",
        "      plt.imshow(convert_to_numpy(img.squeeze() if is_require_squeeze else img), cmap=cmap)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "ibV_lOlnBwe4"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import random\n",
        "from torch import nn\n",
        "\n",
        "try:\n",
        "  from torchsummary import summary\n",
        "except:\n",
        "  !pip install torchsummary\n",
        "  from torchsummary import summary\n",
        "\n",
        "try:\n",
        "  from torchmetrics import Accuracy\n",
        "except:\n",
        "  !pip install torchmetrics\n",
        "  from torchmetrics import Accuracy\n",
        "  \n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "VIlpsN2MErIj"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchvision.datasets import CIFAR100"
      ],
      "metadata": {
        "id": "xbcDv78jE58_"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"data\"\n",
        "\n",
        "train_data = CIFAR100(\n",
        "    root=ROOT_DIR,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = CIFAR100(\n",
        "    root=ROOT_DIR,\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "U4Fvy3a7FReC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a71626-4345-4713-a7d3-0267827f5c59"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_numpy(tensor:torch.Tensor):\n",
        "  \"\"\"\n",
        "  convert tensor to numpy\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return tensor.cpu() if tensor.ndim > 1 else \"tensor ndim must be greater than 1\"\n",
        "      \n",
        "  except:\n",
        "    return \"Something went wrong while converting tensor to numpy\""
      ],
      "metadata": {
        "id": "pT3kZNZGGCgE"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, label=None, cmap:str=\"gray\",figsize:tuple = (4,4), is_squeeze_not_req:bool=False,\n",
        "               is_img_rgb:bool=False):\n",
        "  \"\"\"\n",
        "    show image on the screen\n",
        "    require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  if label is None:\n",
        "    label = \"Image Caption\"\n",
        "  if is_img_rgb:\n",
        "    image = image.permute(1,2,0)\n",
        "  \n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.imshow(convert_to_numpy(image if is_squeeze_not_req else image.squeeze()), cmap=cmap)\n",
        "  plt.title(label)\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "gmnX3OFLNNjO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)\n",
        "len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHN2auuV1cKw",
        "outputId": "4bcbd80b-714f-4ff5-a778-87f4ef5ba5a0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 32\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH, shuffle=True)\n",
        "# print(len(train_dataloader))\n",
        "\n",
        "# X_train , y_train = next(iter(train_dataloader))\n",
        "# sample_index = torch.randint(len(range(BATCH)), size=(1,)).item()\n",
        "# show_image(X_train[sample_index], label=train_classes[y_train[sample_index]])"
      ],
      "metadata": {
        "id": "47ZonIz2JJNR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes = train_data.classes\n",
        "print(f\"type-> {type(train_classes)} len->{len(train_classes)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkkUP_sg2qNd",
        "outputId": "102a1d3e-d40f-41fb-e921-e03663b555b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type-> <class 'list'> len->100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b, (x,y) = next(enumerate(train_dataloader))\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtecwbHU-nFj",
        "outputId": "7d93f969-867c-4201-8b2c-73d4490e9a4c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHpd-SQl4LuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = torch.randint(len(x), size=(1,)).item()\n",
        "show_image(image=x[sample_index], label=train_classes[y[sample_index]], is_img_rgb=True)\n",
        "# show_multi_images(data=x, classes=train_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "xaU4D5LD1w4-",
        "outputId": "ae15e3c1-0246-4527-849c-a7823c71c5dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashtionClassification(nn.Module):\n",
        "  def __init__(self,input_channel,hidden_channel,output_channel):\n",
        "    super().__init__()\n",
        "    self.input_channel = input_channel\n",
        "    self.hidden_channel = hidden_channel\n",
        "    self.output_channel = output_channel\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "                               nn.Conv2d(in_channels=self.input_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(kernel_size=2,padding=1,stride=2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3)\n",
        "                                ,nn.MaxPool2d(kernel_size=2,padding=1,stride=2)\n",
        "    )\n",
        "    self.layer21 = nn.Sequential(\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3)\n",
        "                                ,nn.MaxPool2d(kernel_size=2,padding=1,stride=2)\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "                                nn.Flatten(),\n",
        "                                nn.Linear(in_features=1152, out_features=100),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=100, out_features=self.output_channel)    ,                         \n",
        "                                nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.layer3(self.layer2(self.layer1(x)))\n",
        "\n"
      ],
      "metadata": {
        "id": "pjXr8VdwMUPl"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axX-NK06XQAS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = setup_device()\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HlmqfndDQg9",
        "outputId": "596f6621-6b7c-4bae-e38e-0df78511e587"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "nAHzKnpLbn7S"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0 = change_device(FashtionClassification(input_channel=3,hidden_channel=32,output_channel=len(train_classes)), device=DEVICE)"
      ],
      "metadata": {
        "id": "zLf_WLEUC8aM"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "5Ez84r-mDhg_"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model0, (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AJ5Mx8lLrgI",
        "outputId": "fe92e821-b579-4edc-ceee-2af75314b514"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 30, 30]             896\n",
            "              ReLU-2           [-1, 32, 30, 30]               0\n",
            "            Conv2d-3           [-1, 32, 28, 28]           9,248\n",
            "              ReLU-4           [-1, 32, 28, 28]               0\n",
            "         MaxPool2d-5           [-1, 32, 15, 15]               0\n",
            "            Conv2d-6           [-1, 32, 13, 13]           9,248\n",
            "              ReLU-7           [-1, 32, 13, 13]               0\n",
            "            Conv2d-8           [-1, 32, 11, 11]           9,248\n",
            "         MaxPool2d-9             [-1, 32, 6, 6]               0\n",
            "          Flatten-10                 [-1, 1152]               0\n",
            "           Linear-11                  [-1, 100]         115,300\n",
            "             ReLU-12                  [-1, 100]               0\n",
            "           Linear-13                  [-1, 100]          10,100\n",
            "             ReLU-14                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 154,040\n",
            "Trainable params: 154,040\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.01\n",
            "Params size (MB): 0.59\n",
            "Estimated Total Size (MB): 1.61\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "id": "VRH4rmxKpttB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad35c214-8045-400b-b75d-a565fa646612"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "total_batches = 0\n",
        "loss_metrics = []\n",
        "accuracy_metrics = []\n",
        "\n",
        "accuracy_fn = Accuracy(task=\"multiclass\", num_classes=len(train_classes))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  model0.train()\n",
        "  for batch, (X_data, y_train) in enumerate(train_dataloader):\n",
        "    # print(f\"epoch -> {batch} X_data->{X_data.shape} y_train->{y_train.shape}\")\n",
        "    # break\n",
        "    X_data = change_device(tensor=X_data, device=DEVICE)\n",
        "    y_train = change_device(tensor=y_train, device=DEVICE)\n",
        "\n",
        "    y_pred = model0(X_data)#.to(torch.cuda.FloatTensor))\n",
        "    # print(y_pred.shape)\n",
        "    loss = loss_fn(y_pred,y_train)\n",
        "    acc = accuracy_fn(y_pred,y_train)  \n",
        "    \n",
        "    # if batch%20 == 0:\n",
        "    #   loss_metrics.append(loss.item())\n",
        "    #   accuracy_metrics.append(acc.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  for batch, (X_test, y_test) in enumerate(test_dataloader):\n",
        "    # print(f\"epoch -> {batch} X_test->{X_test.shape} y_test->{y_test.shape}\")\n",
        "    # break\n",
        "    X_test = change_device(tensor=X_test, device=DEVICE)\n",
        "    y_test = change_device(tensor=y_test, device=DEVICE)\n",
        "\n",
        "    y_pred = model0(X_test)\n",
        "    # print(y_pred.shape)\n",
        "    loss_test = loss_fn(y_pred,y_test)\n",
        "    acc_test = accuracy_fn(y_pred,y_test)  \n",
        "    \n",
        "    if batch%100 == 0:\n",
        "      loss_metrics.append(loss.item())\n",
        "      accuracy_metrics.append(acc.item())\n",
        "      print(f\"loss-> {loss:.5f} loss_test->{loss_test:.5f} acc-> {acc} acc_test {acc_test}\")\n",
        "  # print(loss)\n",
        "# print(loss_metrics[-1])\n",
        "# print(accuracy_metrics[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn4ao222IKPo",
        "outputId": "598e12fb-562a-4e40-8a71-3f8cf280622d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss-> 4.32837 loss_test->4.19894 acc-> 0.0 acc_test 0.0625\n",
            "loss-> 4.32837 loss_test->4.38154 acc-> 0.0 acc_test 0.09375\n",
            "loss-> 4.32837 loss_test->4.26242 acc-> 0.0 acc_test 0.125\n",
            "loss-> 4.32837 loss_test->4.16332 acc-> 0.0 acc_test 0.03125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0.eval()\n",
        "with torch.inference_mode():\n",
        "  for batch, (X_test,y_test) in enumerate(test_dataloader):\n",
        "    y_pred_test = model0(change_dtype(X_test))\n",
        "    loss_test = loss_fn(y_pred_test,y_test)\n",
        "    acc = accuracy_fn(y_pred_test,y_test)\n",
        "  print(f\"loss -> {loss_test:.5f} acc->{acc:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL246rHKMAqO",
        "outputId": "42775633-951b-420e-bb59-625d44b32da3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss -> 4.21313 acc->0.18750\n"
          ]
        }
      ]
    }
  ]
}