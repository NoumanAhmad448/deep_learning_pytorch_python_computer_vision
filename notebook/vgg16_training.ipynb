{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def change_device(tensor: torch.Tensor, device=\"cuda\"):\n",
        "  return tensor.to(device)\n",
        "\n",
        "def setup_device():\n",
        "    return \"cuda\" if  torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def change_dtype(tensor:torch.Tensor, dtype=torch.float):\n",
        "  \"\"\"\n",
        "    dtype must have the value mentioned in the torch documentation\n",
        "    e.g. torch.float , torch.LongTensor\n",
        "  \"\"\"\n",
        "  return tensor.to(dtype)\n",
        "def show_image(image, label=None, is_squeeze_req:bool=False):\n",
        "  \"\"\"\n",
        "    show image on the screen\n",
        "    require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  if label is None:\n",
        "    label = \"Image Caption\"\n",
        "  plt.figure(figsize=(4,4))\n",
        "  plt.imshow(convert_to_numpy(image))\n",
        "  plt.title(label)\n",
        "  plt.axis(False)\n",
        "\n",
        "def convert_to_tensor(array, is_change_dtype=False, is_change_device=False, to_device=\"cpu\")->torch.Tensor:\n",
        "  \"\"\"\n",
        "  convert numpy array to tensor\n",
        "  is_change_dtype=True will change the dtype to float\n",
        "  \"\"\"\n",
        "\n",
        "  tensor = None\n",
        "  if torch.is_tensor(array):\n",
        "     tensor = array\n",
        "  else:\n",
        "     tensor = torch.from_numpy(array)\n",
        "  \n",
        "  if is_change_dtype: \n",
        "    tensor = change_dtype(tensor)\n",
        "  \n",
        "  if is_change_device: \n",
        "    tensor = change_device(tensor, device=to_device)\n",
        "  \n",
        "  return tensor\n",
        "\n",
        "# convert_to_tensor(random.rand(2,3),is_change_dtype=True)"
      ],
      "metadata": {
        "id": "vCIJzVZCYz0d"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_numpy(tensor:torch.Tensor):\n",
        "  \"\"\"\n",
        "  convert tensor to numpy\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return tensor.cpu() if tensor.ndim > 1 else \"tensor ndim must be greater than 1\"\n",
        "      \n",
        "  except:\n",
        "    return \"Something went wrong while converting tensor to numpy\""
      ],
      "metadata": {
        "id": "5_qUymunVkQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, label=None, cmap:str=\"gray\",figsize:tuple = (4,4), is_squeeze_not_req:bool=False):\n",
        "  \"\"\"\n",
        "    show image on the screen\n",
        "    require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  if label is None:\n",
        "    label = \"Image Caption\"\n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.imshow(convert_to_numpy(image if is_squeeze_not_req else image.squeeze()), cmap=cmap)\n",
        "  plt.title(label)\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "KaniAV6mVGqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_multi_images(data: torch.Tensor, figsize:tuple=(9,9), rows:int=3, cols:int=3, classes=None,\n",
        "                      cmap=\"gray\", is_require_squeeze:bool=True):\n",
        "  \"\"\"\n",
        "  classes list|dict\n",
        "  require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  DEFAULT_LABEL = None\n",
        "  if classes is None:\n",
        "    DEFAULT_LABEL = \"Image Caption\"\n",
        "  \n",
        "  figure = plt.figure(figsize=figsize)\n",
        "  cols, rows = cols, rows\n",
        "  for i in range(1, cols * rows + 1):\n",
        "      sample_idx = torch.randint(len(data), size=(1,)).item()\n",
        "      img, label = data[sample_idx]\n",
        "      figure.add_subplot(rows, cols, i)\n",
        "      plt.title(DEFAULT_LABEL if classes is None else classes[label])\n",
        "      plt.axis(False)\n",
        "      plt.imshow(convert_to_numpy(img.squeeze() if is_require_squeeze else img), cmap=cmap)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "ibV_lOlnBwe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import random\n",
        "from torch import nn\n",
        "try:\n",
        "  from torchsummary import summary\n",
        "except:\n",
        "  !pip install torchsummary\n",
        "  from torchsummary import summary\n",
        "\n",
        "try:\n",
        "  from torchmetrics import Accuracy\n",
        "except:\n",
        "  !pip install torchmetrics\n",
        "  from torchmetrics import Accuracy\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#new \n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "VIlpsN2MErIj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.device(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vS9bEVg36xo",
        "outputId": "d9a20043-42f4-49ee-bcc5-7314a7d146aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR100"
      ],
      "metadata": {
        "id": "xbcDv78jE58_"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_operation():\n",
        "  return transforms.Compose([\n",
        "          transforms.Resize((256, 256)),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "          transforms.RandomAffine(degrees=40, translate=None, scale=(1, 2), shear=15),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "  ])"
      ],
      "metadata": {
        "id": "wRSQmtgcrTVn"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"data\"\n",
        "\n",
        "train_data = CIFAR100(\n",
        "    root=ROOT_DIR,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = CIFAR100(\n",
        "    root=ROOT_DIR,\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "U4Fvy3a7FReC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608054b1-a0e1-46d9-b1dc-4b3b8b3bf773"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data.transform = transform_operation\n",
        "# test_data.transform = transform_operation"
      ],
      "metadata": {
        "id": "DqJZx-BMuSGW"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_numpy(tensor:torch.Tensor):\n",
        "  \"\"\"\n",
        "  convert tensor to numpy\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return tensor.cpu() if tensor.ndim > 1 else \"tensor ndim must be greater than 1\"\n",
        "      \n",
        "  except:\n",
        "    return \"Something went wrong while converting tensor to numpy\""
      ],
      "metadata": {
        "id": "pT3kZNZGGCgE"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, label=None, cmap:str=\"gray\",figsize:tuple = (4,4), is_squeeze_not_req:bool=False,\n",
        "               is_img_rgb:bool=False):\n",
        "  \"\"\"\n",
        "    show image on the screen\n",
        "    require convert_to_numpy function to be called first\n",
        "  \"\"\"\n",
        "  if label is None:\n",
        "    label = \"Image Caption\"\n",
        "  if is_img_rgb:\n",
        "    image = image.permute(1,2,0)\n",
        "  \n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.imshow(convert_to_numpy(image if is_squeeze_not_req else image.squeeze()), cmap=cmap)\n",
        "  plt.title(label)\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "gmnX3OFLNNjO"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data.classes))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHN2auuV1cKw",
        "outputId": "61cf72b2-5071-4876-8519-1d5266a2f2cd"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 32\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH, shuffle=True)\n",
        "# print(len(train_dataloader))\n",
        "\n",
        "# X_train , y_train = next(iter(train_dataloader))\n",
        "# sample_index = torch.randint(len(range(BATCH)), size=(1,)).item()\n",
        "# show_image(X_train[sample_index], label=train_classes[y_train[sample_index]])"
      ],
      "metadata": {
        "id": "47ZonIz2JJNR"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes = train_data.classes\n",
        "print(f\"type-> {type(train_classes)} len->{len(train_classes)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkkUP_sg2qNd",
        "outputId": "fbcee5d8-f5fd-4892-b7e8-d8eb246c0c94"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type-> <class 'list'> len->100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b, (x,y) = next(enumerate(train_dataloader))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtecwbHU-nFj",
        "outputId": "37c72160-8c9b-4bf9-963a-a88bc28e37a0"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHpd-SQl4LuI"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = torch.randint(len(x), size=(1,)).item()\n",
        "show_image(image=x[sample_index], label=train_classes[y[sample_index]], is_img_rgb=True)\n",
        "# show_multi_images(data=x, classes=train_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "xaU4D5LD1w4-",
        "outputId": "e543e004-4cc3-416d-ec64-c14c24f9c18d"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashtionClassification(nn.Module):\n",
        "  def __init__(self,input_channel,hidden_channel,output_channel):\n",
        "    super().__init__()\n",
        "    self.input_channel = input_channel\n",
        "    self.hidden_channel = hidden_channel\n",
        "    self.output_channel = output_channel\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "                               nn.Conv2d(in_channels=self.input_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(kernel_size=2,padding=1,stride=2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3)\n",
        "                                ,nn.MaxPool2d(kernel_size=2,padding=1,stride=2)\n",
        "    )\n",
        "    self.layer21 = nn.Sequential(\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=self.hidden_channel,out_channels=self.hidden_channel,kernel_size=3)\n",
        "                                ,nn.MaxPool2d(kernel_size=2,padding=1,stride=2)\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "                                nn.Flatten(),\n",
        "                                nn.Linear(in_features=1152, out_features=100),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=100, out_features=self.output_channel)    ,                         \n",
        "                                nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.layer3(self.layer2(self.layer1(x)))\n",
        "\n"
      ],
      "metadata": {
        "id": "pjXr8VdwMUPl"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0 = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "id": "haQ-kCX_eiIl"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model0 = change_device(FashtionClassification(input_channel=3,hidden_channel=32,output_channel=len(train_classes)), device=DEVICE)"
      ],
      "metadata": {
        "id": "zLf_WLEUC8aM"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "5Ez84r-mDhg_"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0 = change_device(tensor=model0,device=DEVICE)"
      ],
      "metadata": {
        "id": "kd-snRFufTHG"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qwC0UDEy3c1",
        "outputId": "ca83bc2f-3e8b-4b1b-b126-e7f661b30922"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model0, (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AJ5Mx8lLrgI",
        "outputId": "a36a67c7-60d8-455c-cfa0-4a65f53825e1"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "              ReLU-2           [-1, 64, 32, 32]               0\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
            "              ReLU-4           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
            "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
            "              ReLU-7          [-1, 128, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
            "              ReLU-9          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
            "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
            "             ReLU-12            [-1, 256, 8, 8]               0\n",
            "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-14            [-1, 256, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-16            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
            "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
            "             ReLU-19            [-1, 512, 4, 4]               0\n",
            "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-21            [-1, 512, 4, 4]               0\n",
            "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-23            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
            "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
            "             ReLU-26            [-1, 512, 2, 2]               0\n",
            "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
            "             ReLU-28            [-1, 512, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
            "             ReLU-30            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
            "           Linear-33                 [-1, 4096]     102,764,544\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                 [-1, 4096]      16,781,312\n",
            "             ReLU-37                 [-1, 4096]               0\n",
            "          Dropout-38                 [-1, 4096]               0\n",
            "           Linear-39                 [-1, 1000]       4,097,000\n",
            "================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.84\n",
            "Params size (MB): 527.79\n",
            "Estimated Total Size (MB): 532.65\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomClassificationModel(nn.Module):\n",
        "  def __init__(self, model_layers:int=1000,output_layer:int=100):\n",
        "    super().__init__()\n",
        "    self.test = 20\n",
        "    self.net = model0\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=model_layers,out_features=output_layer+400),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(in_features=output_layer+400,out_features=output_layer+200),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=output_layer+200,out_features=output_layer)\n",
        "            )\n",
        "    # for p in self.net.parameters():\n",
        "    #   p.requires_grad = False\n",
        "    \n",
        "  def forward(self,x):\n",
        "    return self.classifier(self.net(x))"
      ],
      "metadata": {
        "id": "sBEpvJiHk23N"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model = change_device(CustomClassificationModel(model_layers=1000,output_layer=100), device=\n",
        "                            DEVICE)"
      ],
      "metadata": {
        "id": "QAHLZubnl4jF"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUOnO3o8oIFN",
        "outputId": "a0d4acc0-1500-4a04-81ce-fe32540e5fed"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0.classifier[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GswFAdC9mhZk",
        "outputId": "77b8bfa7-a097-4906-a703-d8733ccbb744"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=4096, out_features=1000, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(train_model, (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdY1XcjrmHM2",
        "outputId": "459fbcf1-5c80-4788-81c4-5d4c709bb7d6"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "              ReLU-2           [-1, 64, 32, 32]               0\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
            "              ReLU-4           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
            "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
            "              ReLU-7          [-1, 128, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
            "              ReLU-9          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
            "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
            "             ReLU-12            [-1, 256, 8, 8]               0\n",
            "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-14            [-1, 256, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-16            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
            "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
            "             ReLU-19            [-1, 512, 4, 4]               0\n",
            "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-21            [-1, 512, 4, 4]               0\n",
            "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-23            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
            "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
            "             ReLU-26            [-1, 512, 2, 2]               0\n",
            "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
            "             ReLU-28            [-1, 512, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
            "             ReLU-30            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
            "           Linear-33                 [-1, 4096]     102,764,544\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                 [-1, 4096]      16,781,312\n",
            "             ReLU-37                 [-1, 4096]               0\n",
            "          Dropout-38                 [-1, 4096]               0\n",
            "           Linear-39                 [-1, 1000]       4,097,000\n",
            "              VGG-40                 [-1, 1000]               0\n",
            "           Linear-41                  [-1, 500]         500,500\n",
            "             ReLU-42                  [-1, 500]               0\n",
            "          Dropout-43                  [-1, 500]               0\n",
            "           Linear-44                  [-1, 300]         150,300\n",
            "             ReLU-45                  [-1, 300]               0\n",
            "          Dropout-46                  [-1, 300]               0\n",
            "           Linear-47                  [-1, 100]          30,100\n",
            "================================================================\n",
            "Total params: 139,038,444\n",
            "Trainable params: 139,038,444\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.87\n",
            "Params size (MB): 530.39\n",
            "Estimated Total Size (MB): 535.27\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmAlyDBsjZd9",
        "outputId": "e16b7683-2649-47bb-d1ca-d2f4c269d6ed"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=1000, out_features=500, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Dropout(p=0.3, inplace=False)\n",
              "  (3): Linear(in_features=500, out_features=300, bias=True)\n",
              "  (4): ReLU()\n",
              "  (5): Dropout(p=0.2, inplace=False)\n",
              "  (6): Linear(in_features=300, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "id": "VRH4rmxKpttB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b83085c8-1da8-40a8-9ff3-5469ffe3cf61"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0 = train_model"
      ],
      "metadata": {
        "id": "OpyUyeQCpyW-"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "total_batches = 0\n",
        "loss_metrics = []\n",
        "accuracy_metrics = []\n",
        "\n",
        "accuracy_fn = change_device(Accuracy(task=\"multiclass\", num_classes=len(train_classes)),device=DEVICE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  model0.train()\n",
        "  for batch, (X_data, y_train) in enumerate(train_dataloader):\n",
        "    # print(f\"epoch -> {batch} X_data->{X_data.shape} y_train->{y_train.shape}\")\n",
        "    # break\n",
        "    X_data = change_device(tensor=X_data, device=DEVICE)\n",
        "    y_train = change_device(tensor=y_train, device=DEVICE)\n",
        "\n",
        "    y_pred = model0(X_data)#.to(torch.cuda.FloatTensor))\n",
        "    # print(y_pred.shape)\n",
        "    loss = loss_fn(y_pred,y_train)\n",
        "    acc = accuracy_fn(y_pred,y_train)\n",
        "    \n",
        "    # if batch%20 == 0:\n",
        "    #   loss_metrics.append(loss.item())\n",
        "    #   accuracy_metrics.append(acc.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  for batch, (X_test, y_test) in enumerate(test_dataloader):\n",
        "    # print(f\"epoch -> {batch} X_test->{X_test.shape} y_test->{y_test.shape}\")\n",
        "    # break\n",
        "    X_test = change_device(tensor=X_test, device=DEVICE)\n",
        "    y_test = change_device(tensor=y_test, device=DEVICE)\n",
        "\n",
        "    y_pred = model0(X_test)\n",
        "    # print(y_pred.shape)\n",
        "    loss_test = loss_fn(y_pred,y_test)\n",
        "    acc_test = accuracy_fn(y_pred,y_test)  \n",
        "    \n",
        "    if batch%100 == 0:\n",
        "      loss_metrics.append(loss.item())\n",
        "      # accuracy_metrics.append(acc.item())\n",
        "      print(f\"loss-> {loss:.5f} loss_test->{loss_test:.5f} acc-> {acc:.5f} acc_test {acc_test:.5f}\")\n",
        "  # print(loss)\n",
        "# print(loss_metrics[-1])\n",
        "# print(accuracy_metrics[-1])"
      ],
      "metadata": {
        "id": "qn4ao222IKPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0.eval()\n",
        "with torch.inference_mode():\n",
        "  for batch, (X_test,y_test) in enumerate(test_dataloader):\n",
        "    y_pred_test = model0(change_dtype(X_test))\n",
        "    loss_test = loss_fn(y_pred_test,y_test)\n",
        "    acc = accuracy_fn(y_pred_test,y_test)\n",
        "  print(f\"loss -> {loss_test:.5f} acc->{acc:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL246rHKMAqO",
        "outputId": "42775633-951b-420e-bb59-625d44b32da3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss -> 4.21313 acc->0.18750\n"
          ]
        }
      ]
    }
  ]
}